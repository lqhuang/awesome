# ML Systems

## Production ready

- [Epistimio/orion](https://github.com/Epistimio/orion): Asynchronous
  Distributed Hyperparameter Optimization. <https://orion.readthedocs.io/>
- [apache/tvm](https://github.com/apache/tvm): Open deep learning compiler stack
  for cpu, gpu and specialized accelerators <https://tvm.apache.org/>
- [ray-project/ray](https://github.com/ray-project/ray): Ray is a unified
  framework for scaling AI and Python applications. Ray consists of a core
  distributed runtime and a toolkit of libraries (Ray AIR) for accelerating ML
  workloads. <https://ray.io/>
- [dmlc/dlpack](https://github.com/dmlc/dlpack): common in-memory tensor
  structure <https://dmlc.github.io/dlpack/latest>
- [aws/deep-learning-containers](https://github.com/aws/deep-learning-containers):
  AWS Deep Learning Containers (DLCs) are a set of Docker images for training
  and serving models in TensorFlow, TensorFlow 2, PyTorch, and MXNet.
  <https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html>

## Frameworks / Libraries

- [flashlight/flashlight](https://github.com/flashlight/flashlight): A C++
  standalone library for machine learning <https://fl.readthedocs.io/en/latest/>

## Autodiff

- [facebookresearch/shumai](https://github.com/facebookresearch/shumai): Fast
  Differentiable Tensor Library in JavaScript and TypeScript with Bun +
  Flashlight
- [google/tangent](https://github.com/google/tangent): Source-to-Source
  Debuggable Derivatives in Pure Python

## Jax eco

- [patrick-kidger/equinox](https://github.com/patrick-kidger/equinox): Callable
  PyTrees and filtered transforms => neural networks in JAX.
  <https://docs.kidger.site/equinox/>
- [google/jaxtyping](https://github.com/google/jaxtyping): Type annotations and
  runtime checking for shape and dtype of JAX arrays, and PyTrees.
- [blackjax-devs/blackjax](https://github.com/blackjax-devs/blackjax): BlackJAX
  is a sampling library designed for ease of use, speed and modularity.
  <https://blackjax-devs.github.io/blackjax/>
- [thomaspinder/GPJax](https://github.com/thomaspinder/GPJax): A didactic
  Gaussian process package for researchers in Jax.
  <https://gpjax.readthedocs.io/en/latest/>
- [patrick-kidger/diffrax](https://github.com/patrick-kidger/diffrax): Numerical
  differential equation solvers in JAX. Autodifferentiable and GPU-capable.
  <https://docs.kidger.site/diffrax/>
- [probml/dynamax](https://github.com/probml/dynamax): State Space Models
  library in JAX <https://probml.github.io/dynamax/>
- [pyro-ppl/numpyro](https://github.com/pyro-ppl/numpyro): Probabilistic
  programming with NumPy powered by JAX for autograd and JIT compilation to
  GPU/TPU/CPU. <https://num.pyro.ai/>
- [jax-ml/oryx](https://github.com/jax-ml/oryx): Oryx is a library for
  probabilistic programming and deep learning built on top of Jax.
  <https://tensorflow.org/probability/oryx>
- [deepmind/distrax](https://github.com/deepmind/distrax): Distrax is a
  lightweight library of probability distributions and bijectors. It acts as a
  JAX-native reimplementation of a subset of TensorFlow Probability (TFP,
  https://www.tensorflow.org/probability), with some new features and emphasis
  on extensibility.
- [deepmind/jraph](https://github.com/deepmind/jraph): A Graph Neural Network
  Library in Jax <https://jraph.readthedocs.io/en/latest/>
- [google/flax](https://github.com/google/flax): Flax is a neural network
  library for JAX that is designed for flexibility.
  <https://flax.readthedocs.io/>
- [google/trax](https://github.com/google/trax): Trax ‚Äî Deep Learning with Clear
  Code and Speed

## Utils

- [patrick-kidger/torchtyping](https://github.com/patrick-kidger/torchtyping):
  Type annotations and dynamic checking for a tensor's shape, dtype, names, etc.
- [tensorchord/envd](https://github.com/tensorchord/envd): üèïÔ∏è Development
  environment for machine learning

## Happy toy

- [minitorch/minitorch](https://github.com/minitorch/minitorch):
- [karpathy/micrograd](https://github.com/karpathy/micrograd): A tiny
  scalar-valued autograd engine and a neural net library on top of it with
  PyTorch-like API
- [geohot/tinygrad](https://github.com/geohot/tinygrad): You like pytorch? You
  like micrograd? You love tinygrad! ‚ù§Ô∏è
- [pranftw/neograd](https://github.com/pranftw/neograd): A deep learning
  framework created from scratch with Python and NumPy
  <https://neograd.readthedocs.io/>
